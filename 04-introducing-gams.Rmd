---
title: "Introducing GAMs"
output: 
  ghdown::github_html_document: 
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  comment = "#>", 
  collapse = TRUE,
  fig.width = 5,
  fig.asp = 0.618)
```

This notebook follows chapter four of Wood (2016).

GAMs add smoothing functions to GLMs.

$$
\begin{align}
	g(\mu_i) =\ &\mathbf{A}_i\boldsymbol{\theta}\ + &\mathrm{[GLM\ part]}\\
	& f_1(x_{1i}) + f_2(x_{2i}) +\ ... &\mathrm{[Smooths\ part]}
\end{align}
\\
$$

To add the smoothing functions we need a way to specify the smoothing functions
and a way to specify the smoothness of the functions.

## Univariate smoothing with tenting functions

To add smoothing functions to GLMs, we need a way to specify *f* so that we
still have a linear model. We do this by choosing a *basis*, a space of
functions containing *f*. For example, if the function is a cubic polynomial,
 $f(x) = \beta_1 + \beta_2x + \beta_3x^2 + \beta_4x^3$, the basis would
be $b_1(x) = 1$ $b_2(x) = x$, etc.

Polynomials are bad for interpolation because they have to have continuous
derivatives so they can oscillate greatly from point to point.

Next part talks about a piecewise linear basis or a tent function. (They look
like pointy splines to me.)

```{r}
library(gamair)
library(ggplot2)

# Data from 19 Volvo engines
data(engine)

ggplot(engine) + 
  aes(x = size, y = wear) + 
  geom_point() +
  labs(x = "Engine capacity", y = "Wear index")
```

Write a function defining a tenting function basis $b_j(x)$:

```{r}
# Get nth tent function from set of n knots
tent_at <- function(x, knots, knot_num) {
  # Defaults to zero
  dj <- knots * 0
  dj[knot_num] <- 1
  approx(knots, dj, x)[["y"]]
}


# Create a tenting function model matrix of x values with xj knots
tent <- function(x, knots) {
  n_knots <- length(knots)
  n_points <- length(x)
  model_matrix <- matrix(NA, n_points, n_knots)
  for (knot_i in seq_len(n_knots)) {
    model_matrix[, knot_i] <- tent_at(x, knots, knot_i)
  }
  model_matrix
}
```

Try six evenly spaced knots over the range of the data.

```{r}
six_knots <- modelr::seq_range(engine$size, n = 6)
tent_basis <- tent(engine$size, six_knots)
round(tent_basis, 2)
```

Fit a model.

```{r}
m <- lm(wear ~ tent_basis - 1, engine)
```

Get the predicted values at 200 points in the x range.

```{r}
pred_range <- modelr::seq_range(engine$size, n = 200)
pred_matrix <- tent(pred_range, six_knots)

# Multiply the six matrix columns by model coefficients
predicted <- data.frame(size = pred_range, wear = pred_matrix %*% coef(m))
```

```{r}
ggplot(engine) + 
  aes(x = size, y = wear) + 
  geom_point() +
  labs(x = "Engine capacity", y = "Wear index") + 
  geom_line(data = predicted)
```

I can use my aweomse polypoly package to help visualize the basis. Here are the
tenting functions at the six knots.

```{r}
library(dplyr, warn.conflicts = FALSE)

unweighted <- polypoly::poly_melt(pred_matrix) %>% 
  left_join(data.frame(observation = 1:200, size = pred_range)) %>% 
  rename(wear = value)

ggplot(unweighted) + 
  aes(x = size, y = wear) + 
  geom_line(aes(group = degree)) + 
  stat_summary(fun.y = sum, geom = "line", color = "blue") + 
  geom_point(data = engine)
```

Here are the model weights applied to each tenting function and how they sum to
the fitted values.

```{r}
weighted <- polypoly::poly_melt(pred_matrix %*% diag(coef(m))) %>% 
  left_join(data.frame(observation = 1:200, size = pred_range)) %>% 
  rename(wear = value)

ggplot(weighted) + 
  aes(x = size, y = wear) + 
  geom_line(aes(group = degree)) + 
  stat_summary(fun.y = sum, geom = "line", color = "blue") + 
  geom_point(data = engine)
```

### Penalize the wiggliness

We add a wiggliness penalty to the least-squares objective to automatically get
the right amount of smoothness. A long derivation shows that wiggliness penalty
can incorporated into the model matrix by augmenting the _y_ and _X_ matrices.

```{r}
fit_penalized_tent <- function(y, x, knots, smoothing) {
  x_mat <- tent(x, knots)
  diffs <- diff(diag(length(knots)), differences = 2)
  # augment the x matrix with smoothing penalties 
  x_mat <- rbind(x_mat, sqrt(smoothing) * diffs)
  y <- c(y, rep(0, nrow(diffs)))
  lm(y ~ x_mat - 1)
}
```

For the sake of illustration, these are the dummy X values that are weighted by
the penalty term.

```{r}
diff(diag(length(six_knots)), differences = 2)
```


We can have lots of knots, and the smoothing penalty will be the main degree of
freedom for fitting the basis. More penalty, more like a straight line. Less
penalty, more wiggle.

```{r}
plot_engine_tent_model <- function(smoothing, knots) {
  knot_breaks <- modelr::seq_range(engine$size, knots)
  m <- fit_penalized_tent(engine$wear, engine$size, knot_breaks, smoothing)
  
  pred_range <- modelr::seq_range(engine$size, 500)
  pred_matrix <- tent(pred_range, knot_breaks)
  predicted <- data.frame(size = pred_range, wear = pred_matrix %*% coef(m))
  
  ggplot(engine) + 
    aes(x = size, y = wear) + 
    geom_point() +
    labs(x = "Engine capacity", y = "Wear index") + 
    geom_line(data = predicted) + 
    labs(caption = sprintf("knots: %s, smoothing: %s", knots, smoothing))
}

plot_engine_tent_model(.01, 20)
plot_engine_tent_model(2, 20)
plot_engine_tent_model(100, 20)
```

We can choose the penalty using cross validation. The book covers ordinary
leave-one-out cross validation and then presents a _generalized cross
validation_ score which uses an average of the influence matrix instead of all
the influence values.

```{r}
gcv_tent <- function(y, x, knots, min_penalty = exp(-9), 
                     max_penalty = exp(11)) {
 n_x <- length(x)
 penalties <- seq(log(min_penalty), log(max_penalty), length.out = 90)
 scores <- rep(NA, length(penalties))
 
  for (penalty_i in seq_along(penalties)) {
    m <- fit_penalized_tent(y, x, knots, exp(penalties[penalty_i]))
    influence_sum <- sum(influence(m)[["hat"]][seq_along(x)])
    resid_sum_sq <- sum((y - fitted(m)[seq_along(x)]) ^ 2)
    scores[penalty_i] <- n_x * resid_sum_sq / (n_x - influence_sum) ^ 2
  }
  data.frame(penalties = exp(penalties), scores)
}

ggplot(gcv_tent(engine$wear, engine$size, knots_20)) + 
  aes(x = log(penalties), y = scores) + 
  geom_line()

plot_engine_tent_model(18, 20)
```

We have to add constraints in order to have more than one smooth function in a
model. Two smoothing functions may differ by a constant, so we need to impose a
constraint to make them identifiable. The book talks about a sum-to-zero
constraint. Apparently, we center the columns by subtracting the column means.
But this reduces the matrix's rank, so we need to drop one of the columns. I
take his word for it.

```{r}
# Removing column means reduces the matrix rank
x_mat_original <- tent(engine$size, six_knots)
Matrix::rankMatrix(x_mat_original)[1]

x_mat_centered <- sweep(x_mat_original, 2, colMeans(x_mat_original))
Matrix::rankMatrix(x_mat_centered)[1]


constrain_tent <- function(x, knots, cmx = NULL, m = 2) {
  n_knots <- length(knots)
  # Remove a column by dropping last column
  X <- tent(x, knots)[, -n_knots]
  D <- diff(diag(n_knots), differences = m)[, -n_knots]
  # Subtract column means
  if (is.null(cmx)) cmx <- colMeans(X)
  X <- sweep(X, 2, cmx)  
  list(X = X, D = D, cmx = cmx)
}
```

Here is a procedure to fit two smooths.

```{r}
library(magrittr)
fit_double_tree <- function(y, x1, x2, smoothing, knots = 10) {
  x1_k <- modelr::seq_range(x1, n = knots)
  x2_k <- modelr::seq_range(x2, n = knots)
  x1_mat_info <- constrain_tent(x1, x1_k)
  x2_mat_info <- constrain_tent(x2, x2_k)
  
  n_d <- nrow(x1_mat_info$D) * 2
  sp <- sqrt(sp)
  X <- cbind(
    c(rep(1, nrow(x1_mat_info$X)), rep(0, n_d)), 
    rbind(x1_mat_info$X, sp[1] * x1_mat_info$D, 0 * x1_mat_info$D))
    )
}
```

